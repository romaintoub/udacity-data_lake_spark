# Project : Data Lake

## Introduction 

Sparkify is a music streaming app. The analytics team want to understand what songs users are listening to. Having a well-structured Postgres database would be the starting point of a good use and analysis of data.

## 1. Goal

Be able to understand what the customer wants and provide him his needs is the mantra of great companies. That's why Sparkify wants to move their data warehouse to a data lake. Their two datasets resides in S3. The first one is the **Song dataset** which is a subset from the Million Song Dataset. The second one is the **Log dataset** which gathered activity logs from the music streaming app based on specified configurations.

In this project, an ETL pipeline is built for a data lake hosted on S3. The process is based on 3 important steps : load data from S3, process the data into analytics tables using Spark, and load them back into S3. 

## 2. Database and project files 

Two datasets are used to build the tables :
### **Song Dataset**
Songs dataset is a subset of [Million Song Dataset](http://millionsongdataset.com/).

Sample Record :
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### **Log Dataset**
Logs dataset is generated by [Event Simulator](https://github.com/Interana/eventsim).

Sample Record :
```
{"artist": null, "auth": "Logged In", "firstName": "Walter", "gender": "M", "itemInSession": 0, "lastName": "Frye", "length": null, "level": "free", "location": "San Francisco-Oakland-Hayward, CA", "method": "GET","page": "Home", "registration": 1540919166796.0, "sessionId": 38, "song": null, "status": 200, "ts": 1541105830796, "userAgent": "\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"", "userId": "39"}
```

## **Project files**

```data``` folder: sample of both datasets extracted in S3.
```etl.py```: This script is the automation process of ```etl.ipynb```, it reads and loads **song_data** and **log_data**.



## Schema for Song Play Analysis

#### Fact Table
songplays - records in event data associated with song plays.

    songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables 
##### users

    user_id, first_name, last_name, gender, level
##### songs

    song_id, title, artist_id, year, duration

##### artists

    artist_id, name, location, lattitude, longitude

##### time

    start_time, hour, day, week, month, year, weekday

## 3. Environment 

* Python 3.6 or above

* PySpark

* AWS Account

## 4. How to Run

1. Create ```dl.cfg``` with your AWS IAM Credentials.

File format for **dl.cfg**
```
$ cat > dl.cfg
[AWS]
AWS_ACCESS_KEY_ID=<your_access_key>
AWS_SECRET_ACCESS_KEY=<your_secret_access>
```

2. Specify _output_data_ path in S3 bucket in ```etl.py```.

3. Run etl.py
```
$ python etl.py
```
